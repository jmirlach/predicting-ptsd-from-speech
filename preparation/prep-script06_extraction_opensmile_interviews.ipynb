{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Per Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import opensmile\n",
    "import os\n",
    "import audiosegment\n",
    "from pydub import AudioSegment\n",
    "import soundfile as sf\n",
    "import tqdm\n",
    "import argparse\n",
    "import audiofile\n",
    "import audtorch\n",
    "import json\n",
    "import glob\n",
    "import torch\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cell parameters\n",
    "src_default = '../data/cropped_data/cropped_interview_data/per_question'\n",
    "src_windowed_default = '../data/cropped_data/cropped_interview_data/per_question/windowed_2000_500'\n",
    "dst_default = '../data/features/features_interview/question_opensmile.csv'\n",
    "dst_windowed_default = '../data/features/features_interview/windowed_opensmile.csv'\n",
    "model_default = 'opensmile'\n",
    "device_default = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "feature_set_default = 'eGeMAPSv02'\n",
    "\n",
    "# Define a function to parse cell parameters\n",
    "def parse_cell_parameters(src=src_default, src_windowed=src_windowed_default, dst=dst_default, dst_windowed=dst_windowed_default, model=model_default, device=device_default, feature_set=feature_set_default):\n",
    "    return {\n",
    "        'src': src,\n",
    "        'src_windowed': src_windowed,\n",
    "        'dst': dst,\n",
    "        'dst_windowed': dst_windowed,\n",
    "        'model': model,\n",
    "        'device': device,\n",
    "        'feature_set': feature_set\n",
    "    }\n",
    "\n",
    "# Parse cell parameters\n",
    "parameters = parse_cell_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst = parameters['dst']\n",
    "if os.path.isfile(dst):\n",
    "    exit()\n",
    "os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "\n",
    "files = glob.glob(os.path.join(parameters['src'], '*.wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_file_features(audio, feature_set, feature_level):\n",
    "    if feature_set=='eGeMAPSv02':\n",
    "        if feature_level=='Functionals':\n",
    "            smile = opensmile.Smile(\n",
    "                feature_set=opensmile.FeatureSet.eGeMAPSv02,\n",
    "                feature_level=opensmile.FeatureLevel.Functionals,\n",
    "            )\n",
    "        elif feature_level=='LLD':\n",
    "            smile = opensmile.Smile(\n",
    "                feature_set=opensmile.FeatureSet.eGeMAPSv02,\n",
    "                feature_level=opensmile.FeatureLevel.LowLevelDescriptors,\n",
    "            )\n",
    "    elif feature_set=='ComParE_2016':\n",
    "        if feature_level=='Functionals':\n",
    "            smile = opensmile.Smile(\n",
    "                feature_set=opensmile.FeatureSet.ComParE_2016,\n",
    "                feature_level=opensmile.FeatureLevel.Functionals,\n",
    "            )\n",
    "        elif feature_level=='LLD':\n",
    "            smile = opensmile.Smile(\n",
    "                feature_set=opensmile.FeatureSet.ComParE_2016,\n",
    "                feature_level=opensmile.FeatureLevel.LowLevelDescriptors,\n",
    "            )\n",
    "\n",
    "    y = smile.process_file(audio)\n",
    "    return pd.DataFrame(y)\n",
    "\n",
    "def extract_opensmile_features(audio_path, feature_set):\n",
    "    feature_df = extract_audio_file_features(audio_path, feature_set=feature_set, feature_level=\"Functionals\")\n",
    "    feature_df.insert(0,'file', audio_path)\n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = []\n",
    "for counter, (file) in tqdm.tqdm(\n",
    "    enumerate(files), \n",
    "    total=len(files), \n",
    "    desc=parameters['model']\n",
    "):\n",
    "    audio, fs = audiofile.read(\n",
    "        file,\n",
    "        always_2d=True\n",
    "    )\n",
    "    audio = audtorch.transforms.Expand(4000)(audio)\n",
    "    audio = torch.from_numpy(audio)\n",
    "    if fs != 16000:\n",
    "        audio = torchaudio.transforms.Resample(fs, 16000)(audio)\n",
    "    if len(audio.shape) == 2:\n",
    "        audio = audio.mean(0)\n",
    "        \n",
    "    features = extract_opensmile_features(audio_path=file, feature_set=parameters['feature_set'])\n",
    "    features_list.append(features)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = pd.concat(features_list)\n",
    "features_df.to_csv(os.path.join(parameters['dst']),index=False)\n",
    "print(features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# With windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_windowed = parameters['dst_windowed']\n",
    "if os.path.isfile(dst_windowed):\n",
    "    exit()\n",
    "os.makedirs(os.path.dirname(dst_windowed), exist_ok=True)\n",
    "\n",
    "files = glob.glob(os.path.join(parameters['src_windowed'], '*.wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = []\n",
    "for counter, (file) in tqdm.tqdm(\n",
    "    enumerate(files), \n",
    "    total=len(files), \n",
    "    desc=parameters['model']\n",
    "):\n",
    "    audio, fs = audiofile.read(\n",
    "        file,\n",
    "        always_2d=True\n",
    "    )\n",
    "    audio = audtorch.transforms.Expand(4000)(audio)\n",
    "    audio = torch.from_numpy(audio)\n",
    "    if fs != 16000:\n",
    "        audio = torchaudio.transforms.Resample(fs, 16000)(audio)\n",
    "    if len(audio.shape) == 2:\n",
    "        audio = audio.mean(0)\n",
    "        \n",
    "    features = extract_opensmile_features(audio_path=file, feature_set=parameters['feature_set'])\n",
    "    features_list.append(features)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = pd.concat(features_list)\n",
    "features_df.to_csv(os.path.join(parameters['dst_windowed']),index=False)\n",
    "print(features_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

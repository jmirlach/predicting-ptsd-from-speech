{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Per Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import audiofile\n",
    "import audtorch\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "from transformers import (\n",
    "    Wav2Vec2Model, \n",
    "    Wav2Vec2Processor,\n",
    "    Wav2Vec2CTCTokenizer,\n",
    "    Wav2Vec2FeatureExtractor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cell parameters\n",
    "src_default = '../data/cropped_data/cropped_interview_data/per_question'\n",
    "dst_default = '../data/features/features_interview/facebook-wav2vec2.csv'\n",
    "dst_wdw_default = '../data/features/features_interview/windowed_facebook-wav2vec2.csv'\n",
    "model_default = 'facebook/wav2vec2-large-xlsr-53-german'\n",
    "device_default = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define a function to parse cell parameters\n",
    "def parse_cell_parameters(src=src_default, dst=dst_default, dst_wdw=dst_wdw_default, model=model_default, device=device_default):\n",
    "    return {\n",
    "        'src': src,\n",
    "        'dst': dst,\n",
    "        'dst_wdw': dst_wdw,\n",
    "        'model': model,\n",
    "        'device': device\n",
    "    }\n",
    "\n",
    "# Parse cell parameters\n",
    "parameters = parse_cell_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst = parameters['dst']\n",
    "if os.path.isfile(dst):\n",
    "    exit()\n",
    "os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "\n",
    "files = glob.glob(os.path.join(parameters['src'], '*.wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = {}\n",
    "with open('vocab.json', 'w') as vocab_file:\n",
    "    json.dump(vocab_dict, vocab_file)\n",
    "tokenizer = Wav2Vec2CTCTokenizer('./vocab.json')\n",
    "tokenizer.save_pretrained('./tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    extractor = Wav2Vec2FeatureExtractor.from_pretrained(parameters['model'])\n",
    "except OSError:\n",
    "    extractor = Wav2Vec2FeatureExtractor(\n",
    "        feature_size=1, \n",
    "        sampling_rate=16000, \n",
    "        padding_value=0.0, \n",
    "        do_normalize=True, \n",
    "        return_attention_mask=True\n",
    "    )\n",
    "processor = Wav2Vec2Processor(feature_extractor=extractor, tokenizer=tokenizer)\n",
    "model = Wav2Vec2Model.from_pretrained(parameters['model']).to(parameters['device'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 768 if 'base' in parameters['model'] else 1024\n",
    "embeddings = torch.zeros(len(files), num_features)\n",
    "for counter, (file) in tqdm.tqdm(\n",
    "    enumerate(files), \n",
    "    total=len(files), \n",
    "    desc=parameters['model']\n",
    "):\n",
    "    audio, fs = audiofile.read(\n",
    "        file,\n",
    "        always_2d=True\n",
    "    )\n",
    "    audio = audtorch.transforms.Expand(4000)(audio)\n",
    "    audio = torch.from_numpy(audio)\n",
    "    if fs != 16000:\n",
    "        audio = torchaudio.transforms.Resample(fs, 16000)(audio)\n",
    "    if len(audio.shape) == 2:\n",
    "        audio = audio.mean(0)\n",
    "    inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        embeddings[counter, :] = model(\n",
    "            inputs.input_values.to(parameters['device']),\n",
    "        )[0].cpu().mean(1).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.DataFrame(\n",
    "    data=embeddings.numpy(),\n",
    "    columns=[f'Neuron_{x}' for x in range(num_features)],\n",
    "    index=pd.Index(files, name='file')\n",
    ").reset_index()\n",
    "features['file'] = features['file'].apply(os.path.basename)\n",
    "features.to_csv(parameters['dst'], index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "# With windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cell parameters\n",
    "src_default = '../data/cropped_data/cropped_interview_data/per_question/windowed_2000_500'\n",
    "dst_default = '../data/features/features_interview/windowed_facebook-wav2vec2.csv'\n",
    "model_default = 'facebook/wav2vec2-large-xlsr-53-german'\n",
    "device_default = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define a function to parse cell parameters\n",
    "def parse_cell_parameters(src=src_default, dst=dst_default, model=model_default, device=device_default):\n",
    "    return {\n",
    "        'src': src,\n",
    "        'dst': dst,\n",
    "        'model': model,\n",
    "        'device': device\n",
    "    }\n",
    "\n",
    "# Parse cell parameters\n",
    "parameters = parse_cell_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst = parameters['dst']\n",
    "if os.path.isfile(dst):\n",
    "    exit()\n",
    "os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "\n",
    "files = glob.glob(os.path.join(parameters['src'], '*.wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = {}\n",
    "with open('vocab.json', 'w') as vocab_file:\n",
    "    json.dump(vocab_dict, vocab_file)\n",
    "tokenizer = Wav2Vec2CTCTokenizer('./vocab.json')\n",
    "tokenizer.save_pretrained('./tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    extractor = Wav2Vec2FeatureExtractor.from_pretrained(parameters['model'])\n",
    "except OSError:\n",
    "    extractor = Wav2Vec2FeatureExtractor(\n",
    "        feature_size=1, \n",
    "        sampling_rate=16000, \n",
    "        padding_value=0.0, \n",
    "        do_normalize=True, \n",
    "        return_attention_mask=True\n",
    "    )\n",
    "processor = Wav2Vec2Processor(feature_extractor=extractor, tokenizer=tokenizer)\n",
    "model = Wav2Vec2Model.from_pretrained(parameters['model']).to(parameters['device'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 768 if 'base' in parameters['model'] else 1024\n",
    "embeddings = torch.zeros(len(files), num_features)\n",
    "for counter, (file) in tqdm.tqdm(\n",
    "    enumerate(files), \n",
    "    total=len(files), \n",
    "    desc=parameters['model']\n",
    "):\n",
    "    audio, fs = audiofile.read(\n",
    "        file,\n",
    "        always_2d=True\n",
    "    )\n",
    "    audio = audtorch.transforms.Expand(4000)(audio)\n",
    "    audio = torch.from_numpy(audio)\n",
    "    if fs != 16000:\n",
    "        audio = torchaudio.transforms.Resample(fs, 16000)(audio)\n",
    "    if len(audio.shape) == 2:\n",
    "        audio = audio.mean(0)\n",
    "    inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        embeddings[counter, :] = model(\n",
    "            inputs.input_values.to(parameters['device']),\n",
    "        )[0].cpu().mean(1).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.DataFrame(\n",
    "    data=embeddings.numpy(),\n",
    "    columns=[f'Neuron_{x}' for x in range(num_features)],\n",
    "    index=pd.Index(files, name='file')\n",
    ").reset_index()\n",
    "features['file'] = features['file'].apply(os.path.basename)\n",
    "features.to_csv(parameters['dst'], index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
